<h2>문제 상황</h2><p>SSE(Server-Sent Events)를 활용해 AI 응답을 단계별로 스트리밍하여 사용자 경험을 개선하는 기능을 구현한 후, 테스트를 하였을 때 답변이 전달되지 않는 문제가 발생하였다.<br />흥미로운 점은, 도메인 주소가 아닌 서버 인스턴스의 IP 주소로 직접 요청을 보낼 경우 모든 단계 응답이 정상적으로 도착했다는 것이다. 이를 통해 코드 로직 자체의 문제가 아니라, 서버 앞단에서 발생하는 문제임을 파악할 수 있었다.<br />현재 서비스는 API 서버 2대를 운영하는 분산 환경으로, 사용자의 요청은 ALB(Application Load Balancer)를 통해 분산되고 있는 상황이었고 서비스 로직 흐름으로는 사용자가 생성 요청(POST)을 보내면 즉시 AI를 호출하여 비동기적으로 응답을 생성하고, 이후 Get 요청을 통해 SSE 채널을 열어 응답을 수신하는 방식으로 코드가 작성되어있다. 하지만 이 과정에서 POST 요청을 처리한 서버와 GET 요청을 처리한 서버가 서로 다른 EC2 인스턴스일 경우, 일부 단계별 응답이 누락되는 현상이 발생하였다.<br />그렇다면, 앞단의 로드 밸런서는 어떤 방식으로 트래픽을 분산하는 것일까? 또, 이 문제는 어떤 방식으로 해결할 수 있었을까? 지금부터 살펴보자.</p><h2>로드 밸런서</h2><h3>로드 밸런서란 무엇일까?</h3><p>서비스가 성장하고 트래픽이 몰리게 된다면 서버 한 대로는 모든 요청을 감당하기 어려운 순간이 발생한다. 더 많은 사용자가 동시에 접속하고 AI 응답처럼 무거운 연산을 처리하는 작업이라면 특정 서버가 금새 포화 상태에 이르게 된다. 이때, 등장하는 것이 바로 로드 밸런서다.&nbsp;<br />로드 밸런서는 말 그대로 "부하를 나누어 주는 장치"이다. 클라이언트가 보내는 요청을 여러 대의 서버로 골고루 분산시켜 주어 특정 서버가 과부하에 걸리지 않도록 하고, 동시에 시스템 전체가 안정적으로 운영될 수 있도록 돕는다. 또한 단순히 분산만 하는 것이 아니라, 서버가 정상인지 확인하는 헬스 체크(health check), 특정 사용자의 요청을 같은 서버로 연결해주는 Sticky Session, 암호화된 트래픽을 대신 해독해주는 TLS 종료 등 다양한 역할을 수행한다. 즉, 로드 밸런서는 단순한 분산 장치가 아니라 서비스 가용성과 안정성을 지탱하는 핵심 인프라 구성 요소이다.</p><h3>트래픽 분산 원리</h3><h4>라운드 로빈(Round-Robin)</h4><p>라운드 로빈 방식은 여러 주체가 차례대로 기회를 갖는 방식으로 운영체제의 CPU 스케쥴링 기법에서도 흔히 사용된다. 해당 방식의 핵심은 우선순위를 두지 않고 순서대로 자원을 할당한다는 점이다.</p><figure class="imageblock widthContent"><span><img height="223" src="https://blog.kakaocdn.net/dn/b8ATxp/btsQEkAKsnN/wIRb5FD0hkMERbWMnk8Qz0/img.png" width="559" /></span></figure>
<p>예를 들어 서버가 A,B,C 3대가 있다면 첫 번째 요청은 A, 두 번째 요청은 B, 세 번째 요청은 C, 네 번째는 다시 A로 돌아오는 식이다. 구현 관점에서는 next = (next+1) % N 같이 인덱스 연산 하나만 기억하면 되기 때문에 상태 관리가 거의 없고 오버헤드도 적다.<br />이러한 단순성 덕분에 서버 성능이 비슷하고 요청당 처리 시간이 크게 차이 나지 않는 환경에서는 효율적이다. 하지만 현실의 서비스 환경에서는 서버 사양이 다르거나 요청마다 처리 부하가 제각각인 경우가 많아 항상 이상적인 결과를 보장하지는 않는다. 이러한 한계를 보완하기 위해 라운드 로빈을 확장한 방식들이 존재한다.</p><h4>가중 라운드 로빈 알고리즘 (Weighted Round Robin)</h4><p>가중 라운드 로빈 알고리즘이란, 각 서버에 가중치를 부여하여 성능이 좋은 서버가 더 많은 요청을 처리하도록 하는 방식이다. 예를 들어서 서버 A의 가중치를 3, 서버 B의 가중치를 1로 설정하면, 요청 4개 중 3개는 A로, 1개는 B로 분배된다.</p><figure class="imageblock widthContent"><span><img height="730" src="https://blog.kakaocdn.net/dn/bAdJQ9/btsQGqsYivP/gE8n6CxY5jw386CkBecyg1/img.png" width="1024" /></span></figure>
<p>서버별 가중치는 CPU, 메모리 스펙이나 네트워크 자원 등을 기준으로 정할 수 있다. 해당 방식은 서버의 처리 능력이 균등하지 않은 때 효과적이지만, 여전히 실시간 부하 상태나 응답 시간까지 고려하지는 못한다는 한계가 존재한다.</p><h4>로드 밸런서와 라운드 로빈</h4><p>로드 밸런서는 바로 이런 라운드로빈 계열 알고리즘을 이용하여 클라이언트의 요청을 여러 서버로 분배한다.<br />로드 밸런서의 핵심 목표는 특정 서버에만 부하가 집중되지 않도록 하는 것이며, 이를 위해 다음과 같은 기능들을 함께 제공한다.</p><ul><li>헬스 체크 (Health Check): 비정상 서버를 자동으로 감지하고 트래픽 분산 대상에서 제외</li><li>세션 고정 (Sticky Session): 동일 클라이언트가 항상 같은 서버로 연결되도록 보장</li><li>TLS 종료 (TLS Termination): 서버 대신 로드 밸런서에서 암호화 연결을 종료해 서버 부하 감소</li><li>콘텐츠 기반 라우팅 (Content-based Routing): 요청 경로나 호스트 헤더를 기준으로 트래픽을 특정 서버 그룹으로 보냄</li></ul><h4>AWS ALB의 동작 방식</h4><p>AWS의 ALB(Application Load Balancer)는 애플리케이션 계층(Layer 7)에서 동작하는 로드 밸런서로, 요청의 호스트 헤더나 경로 같은 콘텐츠를 기반으로 라우팅 결정을 내릴 수 있는 점이 특징이다.&nbsp;<br />ALB의 기본 분배 알고리즘은 라운드 로빈으로, 등록된 대상 서버에 요청을 순차적으로 할당한다. 즉, ALB는 단순히 TCP 연결 단위가 아니라 <b>HTTP 요청 단위로 균등하게 분산하는 구조를 채택</b>하고 있다. 또한, ALB는 라운드 로빈 외에도 최소 미처리 요청(Least Outstanding Requests) 알고리즘을 지원한다. 이 알고리즘은 <b>현재 처리 중인 요청이 가장 적은 서버에 새로운 요청을 할당</b>하기 때문에 요청 처리 시간이 들쑥날쑥한 환경에서는 더 균형 잡힌 분산 효과를 얻을 수 있다.<br />&nbsp;<br />정리하면,&nbsp; AWS ALB는 라운드 로빈을 기본값으로 하되, 서비스 특성에 따라 다른 알고리즘과 세션 고정, 콘텐츠 기반 라우팅 등을 조합하여 다양한 시나리오에 대응할 수 있는 로드 밸런서이다.</p><h2>해결 방법</h2><h3>문제 분석</h3><figure class="imageblock widthContent"><span><img height="328" src="https://blog.kakaocdn.net/dn/wXZVE/btsQFoikV04/d4KgXbgkXICi8P4mrRCi20/img.png" width="750" /></span></figure>
<p>결국 문제 원인은 사용자가 생기부 생성 버튼을 누르면, 클라이언트 측에서&nbsp; 먼저 사용자가 작성한 프롬프트를 보내는 Post 요청을 보내고 바로 이어서 결과를 수신하기 위해 SSE 연결을 위한 Get 요청을 서버 측에 보낸다. 그런데 이 2개의 요청이 ALB의 라운드 로빈 방식에 의해 각각 다른 인스턴스로 분리되어 보내져, AI가 응답을 생성해 단계별 진행 상황과 최종 결과를 SSE 채널로 전송하려 했지만, 해당 서버 인스턴스에는 요청 시 생성된 TaskId에 대응하는 SSE 채널 정보가 존재하지 않아 응답을 수신하지 못하는 문제가 발생한 것이다.<br />어떻게 해결할 수 있을까?</p><h3>1. Sticky Session</h3><p>로드 밸런서는 기본적으로 요청 단위로 트래픽을 분산 시키기 때문에 한 사용자의 Post 요청과 SSE 연결 요청이 서로 다른 서버로 전달될 수 있다. Sticky Session을 활성화하면, 로드 밸런서가 특정 사용자의 세션을 인식하여 동일 사용자의 모든 요청을 같은 서버 인스턴스로 고정해서 전달하게 된다. 이렇게 하면 Post 요청을 처리한 서버가 그대로 해당 사용자의 SSE 연결도 유지할 수 있으므로 TaskId -&gt; SSE 채널 매칭 문제가 발생하지 않게 된다.<br />하지만, 해당 방식은 특정 사용자가 계속 한 서버에 묶이므로 부하가 균등하게 분산되지 못하는 경우가 생길 수 있다. 특히 장시간 유지되는 SSE 연결이 많으면 특정 서버에 트래픽이 몰릴 위험이 있다. 이는 곧, 우리가 ALB를 도입한 본래의 목적, '트래픽을 균등하게 분산하고 안정성을 확보하려는 목적'에 어긋나기 때문에 최종적으로 Sticky Session 방식은 채택하지 않기로 결정했다.</p><h3>2. Message Queue 사용</h3><p>현재 우리의 생기부 생성 AI 파이프라인은 아래와 같다.<br /><span style="color: #9d9d9d;">(단순히 역할을 나눠서 처리하는 것이 멀티 에이전트 기법인 줄 알았는데, 아님을 깨닫게 되어 전체적으로 수정이 필요한 파이프라인이긴 하다..)</span></p><figure class="imageblock widthContent"><span><img height="870" src="https://blog.kakaocdn.net/dn/dHllDp/btsQEaLUD7h/Evz92QzV2JAh7UsYfMpOgK/img.png" width="2048" /></span></figure>
<p>만약, 각각의 인스턴스에 대응하는 AWS SQS를 나누고, 각 TaskId가 어떤 인스턴스에 SSE 채널이 생성되었는지를 저장소에 기록해둔다면, 해당 경로에 맞춰 메시지를 대응되는 메시지 큐로 전송하도록 설계할 수 있다. 이렇게 하면 Lambda에서 진행되는 응답들은 정상적으로 클라이언트에 전달될 수 있다.<br />하지만 이 방식은 관리 포인트가 지나치게 늘어나게 된다. 서버를 증설할 때마다 해당 서버에 대응되는 메시지 큐를 새로 생성하고 관리해야 하기 때문에 확장성 면에서 매우 떨어진다. 또한 현재 1차 응답 생성은 Spring 서버, 즉 API 서버에서 처리되고 있으므로, Spring 서버에서 처리한 응답들은 여전히 ALB의 라운드 로빈 방식에 의해 일부 요청에 포함되지 못하는 한계가 존재한다.</p><h3>3. Redis Pub/Sub : 최종 선택!</h3><h4>Redis Pub/Sub</h4><figure class="imageblock widthContent"><span><img height="503" src="https://blog.kakaocdn.net/dn/xTf8f/btsQFP07b1V/dhUeSOkwoTswzdpOQLIZ2K/img.png" width="1000" /></span></figure>
<p>Redis Pub/Sub은 Redis 서버를 메시지 브로커처럼 활용하는 구조로, 한 쪽에서 메시지를 발행하면 이를 구독하고 있는 모든 클라이언트가 실시간으로 메시지를 받아 처리할 수 있도록 해준다. 즉, 서버가 메시지를 생성하면 해당 메시지가 자동으로 구독 중인 서버와 연결된 SSE 채널로 전달되는 구조를 만들 수 있다. 이 구조 덕분에, 기존의 문제였던 “POST 요청을 처리한 서버와 SSE 연결을 담당하는 서버가 달라 TaskId와 SSE 채널 매칭이 깨지는 문제”를 해결할 수 있다. 모든 서버가 같은 Redis 채널을 구독하고 있기 때문에, 클라이언트가 어느 서버로 SSE 연결을 열든지 관계없이 메시지를 받을 수 있다.</p><h4>해당 방식을 선택한 이유</h4><p>최종적으로 해당 방식을 선택한 이유는 다음과 같다.<br />먼저, 서버를 증설하더라도 추가 설계 없이 그대로 확장이 가능하다. 서버가 늘어나더라도 모든 서버가 동일한 Redis 채널을 구독하기 때문에, 새로운 서버가 들어와도 자동으로 메시지를 받아 처리할 수 있다. Redis Pub/Sub은 메시지가 발행되면 구독중인 모든 서버를 대상으로 브로드캐스트하여 이벤트를 전달하기 때문이다 또한, 우리는 이미 Redis를 리프레시 토큰 저장소로 사용하고 있었기 때문에, 새로운 관리 포인트를 만들 필요가 없었다. 추가적인 메시지 큐나 별도의 브로커를 구축하지 않아도 되므로 시스템 복잡성을 낮출 수 있었고, 운영 부담 역시 최소화할 수 있었다. 즉, 현재 환경에서 가장 간단하면서도 확장성과 관리 효율성을 동시에 확보할 수 있는 방법이라고 판단하였다.&nbsp;</p><h4>앞으로 고민해야 할 지점</h4><p>하지만 Redis Pub/Sub 방식에도 한계가 존재한다. 가장 대표적인 문제는 메시지 유실이다. Redis Pub/Sub은 구독자가 메시지를 받을 수 있는 상태일 때만 메시지를 전달하므로, 서버와의 연결이 일시적으로 끊긴 경우 해당 메시지는 소실될 수 있다. 또한 모든 메시지가 모든 서버 인스턴스로 브로드캐스트되기 때문에, 트래픽이 몰리면 서버의 부하가 증가할 수 있다. 특히 한 서버가 메시지 처리 속도가 느려지면 HOL(Head-of-Line) 블로킹 문제가 발생하여 처리 지연이 생길 수 있다. 즉, 대규모 트래픽 환경에서는 메시지 순서 지연이나 일부 요청 처리 지연이 발생할 수 있다. 현재 우리 서비스는 서버가 감당할 수 없는 수준의 트래픽이 몰리는 상황은 아니므로 당장은 큰 문제가 되지 않지만, 장기적으로는 이러한 문제들에 대해 고민하는 것이 앞으로 남겨진 과제라고 생각한다.<br />추가로, Redis는 메시지 영속화를 지원하는 Redis Streams 기능을 제공하며, 브로드캐스트 시 각 서버에 별도의 메시지 처리 큐를 두어 HOL 블로킹 문제를 완화하는 방법도 있다. 이러한 개선점들은 앞으로 고민해볼 지점인 것 같다.<br />&nbsp;</p>